{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL - Gera Tabela Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "from datetime import datetime\n",
    "import pyspark.sql.functions as f \n",
    "\n",
    "import DadosIO as Db\n",
    "import Function as Ut_f\n",
    "\n",
    "date = dbutils.widgets.get('date')\n",
    "process = 'ETL_TECH_CHALLENGE_4'\n",
    "\n",
    "class ETL():\n",
    "  def __init__(self, _dados_io):\n",
    "    self._dados_io = _dados_io\n",
    "    self._spark_session = _dados_io.spark_session()\n",
    "\n",
    "  def generate_rules(self):\n",
    "    df_schema = self.create_schema()\n",
    "    df_raw = self.load_files(df_schema)\n",
    "    df_acoes_disney = self.select_columns(df_schema, df_raw)\n",
    "    self.create_location(df_acoes_disney)\n",
    "    return df_acoes_disney\n",
    "\n",
    "  def load_files(self, parameters_schema):\n",
    "    path = f\"/mnt/data/raw/projeto_fiap_4tc/{date}/*\"\n",
    "    return self._spark_session.read.format('csv').schema(parameters_schema).option('header', 'true').load(path)\n",
    "  \n",
    "  def create_schema(self):\n",
    "    parameters_schema = StructType([\n",
    "      StructField('Adj Close', FloatType(), True),\n",
    "      StructField('Close', FloatType(), True),\n",
    "      StructField('High', FloatType(), True),\n",
    "      StructField('Low', FloatType(), True),\n",
    "      StructField('Open', FloatType(), True),\n",
    "      StructField('Volume', IntegerType(), True)\n",
    "    ])\n",
    "    return parameters_schema\n",
    "  \n",
    "  def select_columns(self, schema, df_raw):\n",
    "    df_select = df_raw.select(\n",
    "      f.col('Adj Close').alias('adj_cluster'),\n",
    "      f.col('Close').alias('close'),\n",
    "      f.col('High').alias('High'),\n",
    "      f.col('Low').alias('low'),\n",
    "      f.col('Open').alias('open'),\n",
    "      f.col('Volume').alias('volume')\n",
    "    ).withColumn(\"dt_carga\", f.lit(date))\n",
    "    df = df_select.dropDuplicates().na.drop()\n",
    "    return df\n",
    "  \n",
    "  def create_location(self, df_acoes_disney):\n",
    "    df_acoes_disney.createOrReplaceTempView(\"df_extracao_disney\")\n",
    "\n",
    "    tableExists = spark._jsparkSession.catalog().tableExists(\"sandbox.silver_acoes_disney\")\n",
    "\n",
    "    if not tableExists:\n",
    "        spark.sql(\"\"\"\n",
    "        DROP TABLE IF EXISTS sandbox.silver_acoes_disney;\n",
    "        \"\"\")\n",
    "        spark.sql(\"\"\"\n",
    "        CREATE TABLE sandbox.silver_acoes_disney\n",
    "        USING DELTA\n",
    "        PARTITIONED BY (dt_carga)\n",
    "        LOCATION '/mnt/data/sandbox/silver/acoes_disney'\n",
    "        TBLPROPERTIES (\n",
    "        'delta.deletedFileRetentionDuration' = '30 days',\n",
    "        'delta.logRetentionDuration'         = 'interval 4 week'\n",
    "        )\n",
    "        AS SELECT *\n",
    "        FROM  df_extracao_disney\n",
    "        \"\"\")\n",
    "\n",
    "    else:\n",
    "        print(\"Tabela já existe. Considerar inserção de dados ou outra ação.\")\n",
    "\n",
    "dt_inicio = datetime.now()\n",
    "dados_io = Db.DadosIO(process, delta = True)\n",
    "etl = ETL(dados_io)\n",
    "df_acoes_disney = etl.generate_rules()\n",
    "\n",
    "\n",
    "try:\n",
    "    dados_io.gravar_parquet_insert_table_delta(df = df_acoes_disney, \n",
    "                                            nome = f\"sandbox.silver_acoes_disney\", \n",
    "                                            modo = False, \n",
    "                                            ehParticionada=True)\n",
    "except Exception as ex:\n",
    "    dt_fim = datetime.now()\n",
    "    log = \"ERRO: {}\".format(ex)\n",
    "\n",
    "    Ut_f.gera_log_execucao(process,\n",
    "                           dt_inicio.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                           dt_fim.strftime('%Y-%m-%d %H:%M:%S'), str(dt_fim - dt_inicio),\n",
    "                           log)\n",
    "    raise Exception(ex)\n",
    "else:\n",
    "    dt_fim = datetime.now()\n",
    "    log = 'succeeded'\n",
    "    Ut_f.gera_log_execucao(process,\n",
    "                           dt_inicio.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                           dt_fim.strftime('%Y-%m-%d %H:%M:%S'), str(dt_fim - dt_inicio),\n",
    "                           log)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
